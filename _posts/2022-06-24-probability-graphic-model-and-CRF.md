### 1.基础知识

add rule: 
$$
p(x_{1}) = \int p(x_{1}, x_{2})dx_{2}
$$
conditional probability:
$$
p(x_{1}|x_{2}) = \frac{p(x_{1},x_{2})}{p(x_{2})} \\
p(x_{1},x_{2}) = p(x_{1}|x_{2}) \cdot p(x_{2}) = p(x_{2}|x_{1}) \cdot p(x_{1})
$$
chain rule:
$$
p(x_{1},x_{2}, ..., x_{n}) = \prod^{n}_{i=1} p(x_{i}|x_{1},x_{2}, ..., x_{i-1})
$$
Bayes rule:
$$
p(x_{1}|x_{2}) = \frac{p(x_{1},x_{2})}{\int p(x_{1},x_{2}) dx_{1}} = \frac{p(x_{2}|x_{1})p(x_{1})}{\int p(x_{2}|x_{1})p(x_{1}) dx_{1}}
$$

### 2.概率图模型-**probabilistic graphical model** (**PGM**) 

概率图模型（Probabilistic Graphical Model， PGM），简称图模型（Graphical Model，GM），是指一种用图结构来描述多元随机变量之间条件独立性的概率模型（注意条件独立性)。

参数条件独立性假设可以有效减少参数量（组合）。

![](E:\hzx\GitHub\diancieHZX.github.io\assets\graphic model.jpg)

有向图：又称贝叶斯网络，可以分为单一模型和混合模型，单一模型条件依赖的集合就是单个元素，而混合模型条件依赖的集合则是多个元素。复合时序特征则成为时序模型可有马尔科夫链和高斯过程等。混合模型与时间序列复合则有隐马尔科夫模型、卡尔曼滤波、粒子滤波等模型。

贝叶斯网络模型的概率分解，在贝叶斯网络中，变量间存在如下四种关系：

（1）如果两个节点是直接连接的，它们肯定是非条件独立的，是直接因果关系。其中父节点是“因”，子节点是“果”。

（2）间接因果关系（head-to-tail），即图（a）、图（b）：当$X_{2}$已知时, $X_{1}$和$X{3}$为条件独立，即$X{1} \perp X_{3}|X_{2}$。

（3）共因关系（tail-to-tail），即图（c）：当$X_{2}$未知时, $X_{1}$和$X{3}$是不独立的;当$X_{2}$已知时,$X_{1}$和$X{3}$条件独立，即$X{1} \perp X_{3}|X_{2}$ 。

（4）共果关系（head-to-head），即图（d）：当$X_{2}$未知时, $X_{1}$和$X{3}$是独立的;当$X_{2}$已知时,$X_{1}$和$X{3}$不独立。



无向图：使用无向图（Undirected Graph）来描述变量之间的关系（无因果或决定关系），也称为马尔可夫随机场（Markov Random Field，MRF）。常见的有条件随机场、最大熵模型、玻尔兹曼机。 无向图中的一个全连通子图，称为团（Clique），即团内的所有节点之间都连边。

马尔科夫性质：一个变量$X_{k}$在给定他的邻居变量的情况下独立于其他所有变量
$$
p(x_{k}|x_{i\ne k}) = p(x_{k}|x_{i\in Neighbour(k)})
$$

#### 3.条件随机场（CRF）

条件随机场本质上是一种结合鉴别分类器（discriminator）和图形建模优势的方法，它直接对 $p(y|x)$ 条件分布进行建模，事实上多项式逻辑回归模型可以看做是最简单的CRF模型，即只有一个输出变量。CRF模型相比于一般分类器，它考虑了输入序列的上下文，例如对于一段文字的其中一个单词，其前后的单词的条件分布由链式法则限定。因此条件随机场通常用于自然语言处理、时间序列模型、语音语义识别亦或者一般的图或树形结构。

条件随机场的推理指计算 $p(y|x)$ 的边际分布或者计算最大概率 $y^{*} = argmax_{y}p(y|x)$，条件随机场的学习指关注 $p(y|x;\theta)$ 中参数 $\theta$ 的最大似然学习，如果所有节点的概率分布均为指数簇分布，并且在训练期间能够观察到所有节点，那么这个优化为凸优化，可以用梯度下降法或者准牛顿法求解。

根据CRF的定义，给定一个观察序列 $x$，标签序列 $y$ 的概率分布可以写为、
$$
p(y|x,\theta) = \frac{1}{Z(x)}exp(\sum_{j}\theta_{j}F_{j}(y,x))\\
其中，F_{j}(y,x) = \sum_{i=1}f_{j}(y_{i-1},y_{i},x,i) \\
f_{j}(y_{i-1},y_{i},x,i) 可以是状态方程s(y_{i},x,i) = s(y_{i-1},y_{i},x,i)\\
或者转移方程 t(y_{i-1},y_{i},x,i) = =\begin{cases} 
		b(x,i), & y_{i-1} = IN\ and\ y_{i} = NNP \\
		0, & otherwise.
\end{cases} \\
$$
$b(x,i)$ 是某个观测特征的实测值。如果当前状态（在状态函数的情况下）或以前和当前状态（在转换函数的情况下）具有特定的值，那么每个特征函数都具有这些实值观测特征b(x，i)的值。

假设训练数据${(x^{(k)},y^{(k)})}$ 是独立同分布的，（6）在所有训练序列上的乘积，是以参数 $\theta$ 为变量的函数，称为似然函数，其对数形式即为参数的对数似然函数，
$$
\mathcal{L}(\theta) = \sum_{k} [\log \frac{1}{Z(x^{(k)})} + \sum_{j}\theta_{j}F_{j}(y^{(k)}, x^{(k)})]
$$
对（7）对参数 $\theta$ 求导则能计算参数 $\theta$ 的最大似然。